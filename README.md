# NLP-Skip-Gram-Model-for-Word-Embeddings-A-Python-Implementation
Skip-Gram Model word2vec Example - Discover how to apply the skip-gram algorithm in NLP to create word embeddings from a document dataset.

ğŸ“‚ Project Root/
â”œâ”€â”€ ğŸ“ Input/
â”‚   â”œâ”€â”€ config.py          # Configuration file for setting parameters
â”‚   â”œâ”€â”€ Engine.py          # Main engine logic for training the model
â”‚   â””â”€â”€ complaints.csv     # Input dataset for word embeddings
â”‚
â”œâ”€â”€ ğŸ“ Output/
â”‚   â”œâ”€â”€ emb.pkl            # Saved word embeddings
â”‚   â”œâ”€â”€ idx2word.pkl       # Mapping of indices to words
â”‚   â”œâ”€â”€ tokens.pkl         # Tokenized words from the input data
â”‚   â”œâ”€â”€ weights.pkl        # Model weights
â”‚   â””â”€â”€ word2idx.pkl       # Mapping of words to indices
â”‚
â”œ
